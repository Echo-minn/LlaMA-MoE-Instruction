åŸºäºInstructæ¨¡å‹å·²æœ‰åŸºç¡€èƒ½åŠ›ï¼Œæˆ‘ä»¬çš„è®­ç»ƒç›®æ ‡æ˜¯**è®©Expertå­¦ä¼šåˆ†å·¥ä¸“ä¸šåŒ–**

## ğŸ¯ è®­ç»ƒæ–¹æ³•é€‰æ‹©ï¼šSFT vs DPO

### æ¨èï¼š**å…ˆSFTï¼ŒåDPOï¼ˆå¯é€‰ï¼‰** â­â­â­â­â­

#### ä¸ºä»€ä¹ˆå…ˆç”¨SFTï¼Ÿ

```
å½“å‰çŠ¶æ€ï¼š
âœ… Llama-3.2-3B-Instructå·²æœ‰instructionèƒ½åŠ›
âœ… Upcycleåï¼Œæ¯ä¸ªExpertéƒ½æœ‰ç›¸åŒçš„åŸºç¡€èƒ½åŠ›

è®­ç»ƒç›®æ ‡ï¼š
ğŸ¯ è®©8ä¸ªExpertå­¦ä¼šä¸“ä¸šåŒ–åˆ†å·¥
ğŸ¯ Routerå­¦ä¼šä»»åŠ¡åˆ†é…

æœ€ä½³æ–¹æ³•ï¼šSFT
```

**SFTçš„ä½œç”¨**ï¼š
```python
# SFTè®­ç»ƒè¿‡ç¨‹
for batch in diverse_data:
    # Routeræ ¹æ®è¾“å…¥é€‰æ‹©2ä¸ªExpert
    expert1, expert2 = router(input)
    
    # é€‰ä¸­çš„Expertå¤„ç†ä»»åŠ¡å¹¶æ›´æ–°
    output = experts[expert1, expert2](input)
    loss = compute_loss(output, label)
    
    # åªæœ‰è¢«é€‰ä¸­çš„Expertå’ŒRouteræ›´æ–°
    # éšç€è®­ç»ƒï¼Œä¸åŒExpertä¼šä¸“ä¸šåŒ–å¤„ç†ä¸åŒç±»å‹ä»»åŠ¡
```

#### DPOä»€ä¹ˆæ—¶å€™ç”¨ï¼Ÿ

**DPOé€‚ç”¨äº**ï¼š
```
åœºæ™¯ï¼šæ¨¡å‹å·²ç»èƒ½å®Œæˆä»»åŠ¡ï¼Œä½†éœ€è¦å¯¹é½äººç±»åå¥½
- é€‰æ‹©æ›´æœ‰å¸®åŠ©çš„å›å¤
- é€‰æ‹©æ›´å®‰å…¨çš„å›å¤
- é€‰æ‹©æ›´ç¬¦åˆé£æ ¼çš„å›å¤

éœ€è¦ï¼šPreference pairs (å¥½çš„å›å¤ vs åçš„å›å¤)
```

**å¯¹äºMoE Upcyclingåçš„æ¨¡å‹**ï¼š
```
é˜¶æ®µ1: SFT (å¿…éœ€)
  â†’ è®©Expertåˆ†å·¥ä¸“ä¸šåŒ–
  
é˜¶æ®µ2: DPO (å¯é€‰ï¼Œé”¦ä¸Šæ·»èŠ±)
  â†’ è¿›ä¸€æ­¥å¯¹é½åå¥½
```

---

## ğŸ“š æ¨èæ•°æ®é›†

### æ ¸å¿ƒåŸåˆ™ï¼š**å¤šæ ·æ€§ > æ•°é‡**

ä¸ºäº†è®©8ä¸ªExpertå­¦ä¼šä¸“ä¸šåŒ–ï¼Œéœ€è¦**è¦†ç›–ä¸åŒç±»å‹ä»»åŠ¡**çš„æ•°æ®ã€‚

### æ–¹æ¡ˆAï¼šå•ä¸€é«˜è´¨é‡æ•°æ®é›†ï¼ˆå¿«é€Ÿå¼€å§‹ï¼‰â­â­â­â­

#### 1. **Alpaca-GPT4** (æ¨è) â­â­â­â­â­
```yaml
dataset: "vicgalle/alpaca-gpt4"
samples: 52K
quality: é«˜ï¼ˆGPT-4ç”Ÿæˆï¼‰
diversity: éå¸¸å¥½ï¼ˆinstruction, input, outputï¼‰

ä¼˜ç‚¹:
âœ… é«˜è´¨é‡
âœ… ä»»åŠ¡å¤šæ ·
âœ… æ•°æ®å¹²å‡€
âœ… å³å¼€å³ç”¨
```

#### 2. **OpenOrca**
```yaml
dataset: "Open-Orca/OpenOrca"  
samples: 1M+ (å¯å–subset)
quality: é«˜
diversity: æå¥½ï¼ˆå¤šç§æ¨ç†ä»»åŠ¡ï¼‰

ä¼˜ç‚¹:
âœ… åŒ…å«CoTæ¨ç†
âœ… ä»»åŠ¡æå…¶å¤šæ ·
âœ… é€‚åˆexpertä¸“ä¸šåŒ–
```

### æ–¹æ¡ˆBï¼šæ··åˆæ•°æ®é›†ï¼ˆæœ€ä½³æ•ˆæœï¼‰â­â­â­â­â­

**æ¨èé…ç½®**ï¼šæ··åˆä¸åŒé¢†åŸŸï¼Œè®©Expertè‡ªç„¶åˆ†å·¥

```yaml
# configs/data_mix.yaml
datasets:
  # 1. é€šç”¨æŒ‡ä»¤ (30%)
  - name: "vicgalle/alpaca-gpt4"
    weight: 0.3
    samples: 15000
    type: general_instruction
  
  # 2. å¯¹è¯/åŠ©æ‰‹ (25%)
  - name: "OpenAssistant/oasst2"
    weight: 0.25
    samples: 12000
    type: conversation
  
  # 3. ä»£ç  (20%)
  - name: "iamtarun/python_code_instructions_18k_alpaca"
    weight: 0.2
    samples: 10000
    type: code
  
  # 4. æ¨ç†/æ•°å­¦ (15%)
  - name: "gsm8k"
    weight: 0.15
    samples: 7500
    type: reasoning
  
  # 5. åˆ›æ„å†™ä½œ (10%)
  - name: "garage-bAInd/Open-Platypus"
    weight: 0.1
    samples: 5000
    type: creative

# Total: ~50K samples
# Training time: ~2-3 hours
```

**ä¸ºä»€ä¹ˆè¿™ä¸ªé…ç½®å¥½ï¼Ÿ**
```
ä¸åŒç±»å‹æ•°æ® â†’ ä¸åŒExpertä¸“ä¸šåŒ–

å¯èƒ½çš„åˆ†å·¥ï¼ˆè‡ªç„¶æ¶Œç°ï¼‰ï¼š
Expert 1,2: é€šç”¨æŒ‡ä»¤ç†è§£
Expert 3,4: å¯¹è¯å’ŒåŠ©æ‰‹ä»»åŠ¡
Expert 5,6: ä»£ç ç›¸å…³ä»»åŠ¡
Expert 7,8: æ¨ç†å’Œæ•°å­¦ä»»åŠ¡
```

### æ–¹æ¡ˆCï¼šå°è§„æ¨¡å¿«é€Ÿæµ‹è¯• â­â­â­

```yaml
dataset: "tatsu-lab/alpaca"
samples: 52K
training: ~1 hour

ç›®çš„: å¿«é€ŸéªŒè¯pipeline
```

---

## ğŸ“Š å…·ä½“æ¨è

### ğŸ¥‡ æœ€æ¨èï¼šæ··åˆæ•°æ®é›†

```python
# åˆ›å»ºæ··åˆæ•°æ®é…ç½®
{
    "general": {
        "data": "vicgalle/alpaca-gpt4",
        "samples": 15000,
        "description": "é€šç”¨instruction following"
    },
    "conversation": {
        "data": "OpenAssistant/oasst2", 
        "samples": 12000,
        "description": "å¤šè½®å¯¹è¯"
    },
    "code": {
        "data": "iamtarun/python_code_instructions_18k_alpaca",
        "samples": 10000,
        "description": "ä»£ç ç”Ÿæˆ"
    },
    "reasoning": {
        "data": "gsm8k",
        "samples": 8000,
        "description": "æ•°å­¦æ¨ç†"
    }
}

Total: 45K samples
Expected training: 2-3 hours
```

**é¢„æœŸæ•ˆæœ**ï¼š
- âœ… Expertè‡ªç„¶åˆ†å·¥å¤„ç†ä¸åŒä»»åŠ¡
- âœ… Routerå­¦ä¼šä»»åŠ¡åˆ†ç±»
- âœ… æ¨¡å‹åœ¨å„é¢†åŸŸéƒ½è¡¨ç°è‰¯å¥½

---

## ğŸ”„ è®­ç»ƒæµç¨‹å»ºè®®

### é˜¶æ®µ1ï¼šSFT - Expertä¸“ä¸šåŒ– (å¿…éœ€)

```bash
# ä½¿ç”¨æ··åˆæ•°æ®é›†SFTè®­ç»ƒ
python scripts/train_sft.py \
    --model_name_or_path models/Llama-3.2-3B-Instruct-MoE-8x \
    --data_path "æ··åˆæ•°æ®é›†" \
    --num_experts_to_train 8 \
    --max_steps 5000
```

**æ—¶é—´**ï¼š2-3å°æ—¶  
**æ•ˆæœ**ï¼šExpertå­¦ä¼šåˆ†å·¥

### é˜¶æ®µ2ï¼šDPO - åå¥½å¯¹é½ (å¯é€‰)

```bash
# å¦‚æœæƒ³è¿›ä¸€æ­¥ä¼˜åŒ–
python scripts/train_dpo.py \
    --model_name_or_path outputs/sft_checkpoint \
    --data_path "HuggingFaceH4/ultrafeedback_binarized" \
    --max_steps 1000
```

**æ—¶é—´**ï¼š1å°æ—¶  
**æ•ˆæœ**ï¼šå›å¤æ›´ç¬¦åˆäººç±»åå¥½

---

## ğŸ’¡ å®ç”¨å»ºè®®

### å¿«é€Ÿå¼€å§‹æ–¹æ¡ˆ

**Day 1: éªŒè¯pipeline**
```bash
# 1. ä¸‹è½½æ¨¡å‹
python scripts/convert_llama3b_to_moe.py

# 2. å°æ•°æ®é›†æµ‹è¯•
data: "tatsu-lab/alpaca" (å‰5000æ ·æœ¬)
time: 30åˆ†é’Ÿ
goal: éªŒè¯ä»£ç æ­£å¸¸
```

**Day 2: æ­£å¼è®­ç»ƒ**
```bash
# ä½¿ç”¨æ··åˆæ•°æ®é›†
data: æ··åˆé…ç½® (45K samples)
time: 2-3å°æ—¶
goal: è®­ç»ƒå®Œæ•´æ¨¡å‹
```

**Day 3: è¯„ä¼° & å¯é€‰DPO**
```bash
# è¯„ä¼°æ•ˆæœ
# å¦‚æœæ»¡æ„ â†’ å®Œæˆ
# å¦‚æœéœ€è¦ä¼˜åŒ– â†’ DPO
```

---

## ğŸ“ æ•°æ®é›†å‡†å¤‡ç¤ºä¾‹

è®©æˆ‘åˆ›å»ºä¸€ä¸ªæ•°æ®æ··åˆè„šæœ¬ï¼š

```python
# scripts/prepare_mixed_dataset.py
from datasets import load_dataset, concatenate_datasets

def create_mixed_dataset():
    datasets_config = [
        ("vicgalle/alpaca-gpt4", 15000),
        ("OpenAssistant/oasst2", 12000),
        ("iamtarun/python_code_instructions_18k_alpaca", 10000),
        ("gsm8k", 8000),
    ]
    
    mixed = []
    for name, samples in datasets_config:
        ds = load_dataset(name, split=f"train[:{samples}]")
        mixed.append(ds)
    
    final_dataset = concatenate_datasets(mixed)
    final_dataset.shuffle(seed=42)
    final_dataset.save_to_disk("data/mixed_instruction")
    
    return final_dataset
```

---

## ğŸ¯ æ€»ç»“

| é—®é¢˜ | ç­”æ¡ˆ |
|------|------|
| **è®­ç»ƒæ–¹æ³•** | **SFT first** (DPOå¯é€‰) |
| **æœ€ä½³æ•°æ®é›†** | **æ··åˆæ•°æ®é›†** (45K samples) |
| **å¿«é€Ÿå¼€å§‹** | Alpaca-GPT4 (52K) |
| **è®­ç»ƒæ—¶é—´** | 2-3å°æ—¶ |
| **å…³é”®ç›®æ ‡** | Expertä¸“ä¸šåŒ–åˆ†å·¥ |

**æ ¸å¿ƒç­–ç•¥**ï¼š
```
å¤šæ ·åŒ–æ•°æ® â†’ Expertä¸“ä¸šåŒ– â†’ å¼ºå¤§çš„MoEæ¨¡å‹
```

è¦æˆ‘å¸®ä½ å‡†å¤‡æ··åˆæ•°æ®é›†çš„åŠ è½½è„šæœ¬å—ï¼ŸğŸš€