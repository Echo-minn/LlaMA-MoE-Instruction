#!/usr/bin/env python3
"""
ä¼°ç®—ä¸åŒMoEé…ç½®çš„æ˜¾å­˜å ç”¨
"""

def estimate_memory(base_params_b, num_experts, expert_size_b, 
                    use_qlora=True, batch_size=4, seq_length=1024):
    """
    ä¼°ç®—è®­ç»ƒæ˜¾å­˜å ç”¨
    
    å‚æ•°:
        base_params_b: Baseæ¨¡å‹å‚æ•°é‡(B)
        num_experts: Expertæ•°é‡
        expert_size_b: æ¯ä¸ªExpertå‚æ•°é‡(B)
        use_qlora: æ˜¯å¦ä½¿ç”¨QLoRA
        batch_size: æ¯GPUçš„batch size
        seq_length: åºåˆ—é•¿åº¦
    """
    print(f"\n{'='*70}")
    print(f"é…ç½®: {base_params_b}B Base + {num_experts} Experts Ã— {expert_size_b}B")
    print(f"è®­ç»ƒ: {'QLoRA' if use_qlora else 'Full LoRA'}, batch={batch_size}, seq_len={seq_length}")
    print(f"{'='*70}")
    
    # æ€»å‚æ•°é‡
    total_params_b = base_params_b + num_experts * expert_size_b
    print(f"\nğŸ“Š å‚æ•°é‡:")
    print(f"  Baseæ¨¡å‹:     {base_params_b:.1f}B")
    print(f"  {num_experts} Experts:   {num_experts * expert_size_b:.1f}B ({expert_size_b:.2f}B each)")
    print(f"  æ€»è®¡:         {total_params_b:.1f}B")
    
    # æ¿€æ´»å‚æ•°ï¼ˆæ¯æ¬¡2ä¸ªexpertï¼‰
    active_params_b = base_params_b + 2 * expert_size_b
    print(f"  æ¿€æ´»å‚æ•°:     {active_params_b:.1f}B (æ¯æ¬¡æ¨ç†)")
    
    # æ˜¾å­˜è®¡ç®—
    print(f"\nğŸ’¾ æ˜¾å­˜å ç”¨ä¼°ç®— (å•GPU):")
    
    # 1. æ¨¡å‹å‚æ•°
    if use_qlora:
        model_memory_gb = total_params_b * 0.5  # 4-bit = 0.5 bytes per param
        param_note = "4-bité‡åŒ–"
    else:
        model_memory_gb = total_params_b * 2  # bf16 = 2 bytes per param
        param_note = "bf16"
    print(f"  æ¨¡å‹å‚æ•° ({param_note}):     {model_memory_gb:.2f} GB")
    
    # 2. LoRAé€‚é…å™¨
    lora_params_b = total_params_b * 0.02  # å‡è®¾LoRAæ˜¯2%å‚æ•°é‡
    lora_memory_gb = lora_params_b * 2  # bf16
    print(f"  LoRAé€‚é…å™¨ (bf16):      {lora_memory_gb:.2f} GB")
    
    # 3. OptimizerçŠ¶æ€ï¼ˆåªä¼˜åŒ–LoRAï¼‰
    optimizer_memory_gb = lora_memory_gb * 2  # AdamWéœ€è¦2å€å‚æ•°å†…å­˜
    print(f"  OptimizerçŠ¶æ€:         {optimizer_memory_gb:.2f} GB")
    
    # 4. æ¢¯åº¦
    gradient_memory_gb = lora_memory_gb
    print(f"  æ¢¯åº¦ç¼“å­˜:              {gradient_memory_gb:.2f} GB")
    
    # 5. æ¿€æ´»å€¼ï¼ˆä¸batch sizeå’Œseq lengthç›¸å…³ï¼‰
    # ç²—ç•¥ä¼°ç®—: æ¯ä¸ªtokenæ¯Bå‚æ•°çº¦éœ€è¦ 4 bytes (bf16 ä¸­é—´æ¿€æ´»)
    activation_memory_gb = active_params_b * batch_size * seq_length * 4 / 1e9
    print(f"  æ¿€æ´»å€¼ (batch={batch_size}):     {activation_memory_gb:.2f} GB")
    
    # 6. KV cache
    hidden_size = int(base_params_b * 1000)  # ç²—ç•¥ä¼°ç®—
    kv_memory_gb = 2 * batch_size * seq_length * hidden_size * 2 / 1e9
    print(f"  KV cache:              {kv_memory_gb:.2f} GB")
    
    # 7. å…¶ä»–å¼€é”€
    other_memory_gb = 1.0
    print(f"  å…¶ä»–å¼€é”€:              {other_memory_gb:.2f} GB")
    
    # æ€»è®¡
    total_memory_gb = (model_memory_gb + lora_memory_gb + optimizer_memory_gb + 
                       gradient_memory_gb + activation_memory_gb + kv_memory_gb + 
                       other_memory_gb)
    
    print(f"\n  {'â”€'*66}")
    print(f"  æ€»è®¡ (å•GPUå³°å€¼):      {total_memory_gb:.2f} GB")
    
    # ZeRO-2åˆ†å¸ƒå¼
    print(f"\nğŸ”§ ZeRO-2 åˆ†å¸ƒå¼ (4 GPUs):")
    # ZeRO-2åˆ†ç‰‡optimizerå’Œgradient
    per_gpu_memory_gb = (model_memory_gb + lora_memory_gb + 
                         optimizer_memory_gb/4 + gradient_memory_gb/4 +
                         activation_memory_gb + kv_memory_gb + other_memory_gb)
    print(f"  æ¯å¡æ˜¾å­˜:              {per_gpu_memory_gb:.2f} GB")
    print(f"  4å¡æ€»æ˜¾å­˜:             {per_gpu_memory_gb * 4:.2f} GB")
    
    # åˆ¤æ–­å¯è¡Œæ€§
    print(f"\nâœ… å¯è¡Œæ€§åˆ†æ (æ¯å¡40GB):")
    if per_gpu_memory_gb < 30:
        status = "âœ… ç»°ç»°æœ‰ä½™"
        detail = f"è¿˜å‰© {40 - per_gpu_memory_gb:.1f}GBï¼Œå¯å¢å¤§batch size"
    elif per_gpu_memory_gb < 38:
        status = "âœ… å®Œå…¨å¯è¡Œ"
        detail = f"è¿˜å‰© {40 - per_gpu_memory_gb:.1f}GB"
    elif per_gpu_memory_gb < 40:
        status = "ğŸŸ¡ å¯è¡Œä½†ç´§å¼ "
        detail = "å»ºè®®å‡å°batch sizeæˆ–å¯ç”¨æ›´å¤šä¼˜åŒ–"
    else:
        status = "âŒ å¯èƒ½OOM"
        detail = "éœ€è¦å‡å°batch sizeæˆ–ä½¿ç”¨æ›´å¤šä¼˜åŒ–"
    
    print(f"  {status}")
    print(f"  {detail}")
    
    # é€Ÿåº¦ä¼°ç®—
    print(f"\nâš¡ è®­ç»ƒé€Ÿåº¦ä¼°ç®—:")
    # ç®€å•æ¨¡å‹: æ—¶é—´ âˆ (total_params)^1.3 Ã— seq_length^1.5 / batch_size
    base_time = (total_params_b ** 1.3) * (seq_length / 1000) ** 1.5 / batch_size * 0.5
    print(f"  æ¯iteration:           ~{base_time:.1f}s")
    print(f"  2000 steps:            ~{base_time * 2000 / 3600:.1f} å°æ—¶")
    
    return per_gpu_memory_gb

print("="*70)
print("Llama-3B-MoE æ˜¾å­˜å ç”¨ä¼°ç®—")
print("="*70)

# é…ç½®1: 3B + 4 experts
mem1 = estimate_memory(
    base_params_b=3.0,
    num_experts=4,
    expert_size_b=0.75,
    use_qlora=True,
    batch_size=4,
    seq_length=1024
)

# é…ç½®2: 3B + 8 experts
mem2 = estimate_memory(
    base_params_b=3.0,
    num_experts=8,
    expert_size_b=0.75,
    use_qlora=True,
    batch_size=4,
    seq_length=1024
)

# å¯¹æ¯”
print(f"\n{'='*70}")
print("ğŸ“Š å¯¹æ¯”æ€»ç»“")
print(f"{'='*70}")
print(f"\né…ç½®å¯¹æ¯”:")
print(f"  {'é…ç½®':<20} {'æ¯å¡æ˜¾å­˜':<15} {'è®­ç»ƒé€Ÿåº¦':<15} {'æ¨èåº¦'}")
print(f"  {'-'*66}")
print(f"  {'3B + 4 experts':<20} {mem1:.1f} GB{' '*7} {'æ›´å¿«':<15} â­â­â­â­â­")
print(f"  {'3B + 8 experts':<20} {mem2:.1f} GB{' '*7} {'ç¨æ…¢':<15} â­â­â­â­")

print(f"\nğŸ’¡ å»ºè®®:")
print(f"  â€¢ ç›®æ ‡æ˜¯Instruction Following â†’ é€‰æ‹© 3B+4experts")
print(f"  â€¢ è¿½æ±‚æœ€å¼ºå¤šä»»åŠ¡èƒ½åŠ› â†’ é€‰æ‹© 3B+8experts")
print(f"  â€¢ ä¸¤è€…éƒ½å¯åœ¨4å¡40GBä¸Šè®­ç»ƒ âœ…")
print(f"\n{'='*70}")

