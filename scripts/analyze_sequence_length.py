#!/usr/bin/env python3
"""
å¿«é€Ÿåˆ†ææ•°æ®é›†çš„åºåˆ—é•¿åº¦åˆ†å¸ƒ
"""

import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from transformers import AutoTokenizer
from datasets import load_dataset
import numpy as np

# é…ç½®
MODEL_PATH = "models/Llama-3.1-8B-MoE-Upcycled"
DATA_PATH = "facebook/natural_reasoning"
SAMPLE_SIZE = 1000  # åˆ†æå‰1000ä¸ªæ ·æœ¬

print("=" * 80)
print("åºåˆ—é•¿åº¦åˆ†æ")
print("=" * 80)

# åŠ è½½tokenizer
print(f"\nğŸ“¥ åŠ è½½ Tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_PATH,
    padding_side="right",
    use_fast=True,
    trust_remote_code=True
)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
print(f"âœ… TokenizeråŠ è½½æˆåŠŸ")

# åŠ è½½æ•°æ®é›†
print(f"\nğŸ“¥ åŠ è½½æ•°æ®é›†ï¼ˆå‰{SAMPLE_SIZE}ä¸ªæ ·æœ¬ï¼‰...")
dataset = load_dataset(DATA_PATH, split=f"train[:{SAMPLE_SIZE}]")
print(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ: {len(dataset)} ä¸ªæ ·æœ¬")

# åˆ†æé•¿åº¦
print(f"\nğŸ“Š åˆ†æåºåˆ—é•¿åº¦...")
instruction_lengths = []
response_lengths = []
total_lengths = []

for i, sample in enumerate(dataset):
    if i % 100 == 0:
        print(f"  å¤„ç†è¿›åº¦: {i}/{len(dataset)}", end="\r")
    
    # æå–questionå’Œresponse
    question = sample.get("question", "")
    
    response = ""
    if "responses" in sample:
        resp_data = sample["responses"]
        if isinstance(resp_data, list) and len(resp_data) > 0:
            first_response = resp_data[0]
            if isinstance(first_response, dict) and "response" in first_response:
                response = first_response["response"]
            else:
                response = str(first_response)
    elif "reference_answer" in sample:
        response = sample["reference_answer"]
    
    # Tokenize
    instruction_text = f"### Instruction:\n{question}\n\n### Response:\n"
    instruction_tokens = tokenizer(instruction_text, add_special_tokens=True)["input_ids"]
    response_tokens = tokenizer(response + tokenizer.eos_token, add_special_tokens=False)["input_ids"]
    
    instruction_lengths.append(len(instruction_tokens))
    response_lengths.append(len(response_tokens))
    total_lengths.append(len(instruction_tokens) + len(response_tokens))

print(f"\n  å¤„ç†è¿›åº¦: {len(dataset)}/{len(dataset)} âœ…")

# ç»Ÿè®¡åˆ†æ
def print_stats(name, lengths):
    arr = np.array(lengths)
    print(f"\n{'='*60}")
    print(f"{name}")
    print(f"{'='*60}")
    print(f"  æ ·æœ¬æ•°é‡: {len(arr)}")
    print(f"  å¹³å‡å€¼:   {arr.mean():.1f} tokens")
    print(f"  ä¸­ä½æ•°:   {np.median(arr):.1f} tokens")
    print(f"  æœ€å°å€¼:   {arr.min()} tokens")
    print(f"  æœ€å¤§å€¼:   {arr.max()} tokens")
    print(f"  æ ‡å‡†å·®:   {arr.std():.1f}")
    print(f"\n  ç™¾åˆ†ä½æ•°:")
    for p in [50, 75, 90, 95, 99]:
        print(f"    {p}%:  {np.percentile(arr, p):.0f} tokens")
    
    # é•¿åº¦åˆ†å¸ƒ
    print(f"\n  é•¿åº¦åˆ†å¸ƒ:")
    bins = [0, 256, 512, 1024, 1536, 2048, 3072, 4096, float('inf')]
    bin_labels = ['<256', '256-512', '512-1024', '1024-1536', '1536-2048', '2048-3072', '3072-4096', '>4096']
    for i in range(len(bins)-1):
        count = np.sum((arr >= bins[i]) & (arr < bins[i+1]))
        pct = count / len(arr) * 100
        bar = 'â–ˆ' * int(pct / 2)
        print(f"    {bin_labels[i]:12s}: {count:4d} ({pct:5.1f}%) {bar}")

print_stats("Instruction é•¿åº¦", instruction_lengths)
print_stats("Response é•¿åº¦", response_lengths)
print_stats("Total é•¿åº¦ (Instruction + Response)", total_lengths)

# æ¨èmax_length
print(f"\n{'='*80}")
print("ğŸ’¡ æ¨èçš„ max_seq_length è®¾ç½®")
print(f"{'='*80}")

total_arr = np.array(total_lengths)
for coverage in [90, 95, 99]:
    percentile_val = np.percentile(total_arr, coverage)
    truncated = np.sum(total_arr > percentile_val)
    print(f"  max_length = {int(percentile_val):4d}  â†’  è¦†ç›– {coverage}% æ ·æœ¬ (æˆªæ–­ {truncated} ä¸ªæ ·æœ¬)")

print(f"\nå½“å‰è®¾ç½®: max_length = 2048")
covered = np.sum(total_arr <= 2048)
coverage_pct = covered / len(total_arr) * 100
truncated = len(total_arr) - covered
print(f"  âœ“ è¦†ç›– {covered}/{len(total_arr)} ä¸ªæ ·æœ¬ ({coverage_pct:.1f}%)")
print(f"  âœ— éœ€è¦æˆªæ–­ {truncated} ä¸ªæ ·æœ¬ ({100-coverage_pct:.1f}%)")

if coverage_pct >= 95:
    print(f"\nâœ… å½“å‰max_length=2048 å·²ç»è¦†ç›–95%+æ ·æœ¬ï¼Œè®¾ç½®åˆç†ï¼")
elif coverage_pct >= 90:
    print(f"\nğŸŸ¡ å½“å‰max_length=2048 è¦†ç›–90-95%æ ·æœ¬ï¼ŒåŸºæœ¬å¤Ÿç”¨ã€‚")
    recommended = int(np.percentile(total_arr, 95))
    print(f"   å¦‚æœæƒ³è¦†ç›–æ›´å¤šï¼Œå»ºè®®å¢åŠ åˆ° {recommended}")
else:
    print(f"\nğŸ”´ å½“å‰max_length=2048 è¦†ç›–ä¸è¶³90%æ ·æœ¬ï¼")
    recommended = int(np.percentile(total_arr, 95))
    print(f"   å»ºè®®å¢åŠ åˆ°è‡³å°‘ {recommended} ä»¥è¦†ç›–95%æ ·æœ¬")

# å¦‚æœå¤ªå¤šæ ·æœ¬è¢«æˆªæ–­
avg_length = total_arr.mean()
if avg_length < 1024:
    print(f"\nğŸ’° ä¼˜åŒ–å»ºè®®: å¹³å‡é•¿åº¦ä»…{avg_length:.0f}ï¼Œå¯ä»¥è€ƒè™‘é™ä½max_lengthåˆ°1024æˆ–1536æ¥:")
    print(f"   â€¢ èŠ‚çœæ˜¾å­˜ (å¯ä»¥å¢å¤§batch_size)")
    print(f"   â€¢ åŠ å¿«è®­ç»ƒé€Ÿåº¦ (å‡å°‘padding)")

print(f"\n{'='*80}")

