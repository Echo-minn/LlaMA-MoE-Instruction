base_model_id: meta-llama/Llama-3.2-3B-Instruct
load_in_4bit: true
bnb_4bit_compute_dtype: float16
bnb_4bit_use_double_quant: true
bnb_4bit_quant_type: nf4
gradient_checkpointing: true
gradient_checkpointing_use_reentrant: false
use_flash_attention_2: true
lora:
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules: null  # null => let PEFT pick common linear modules
  bias: none
  task_type: CAUSAL_LM

